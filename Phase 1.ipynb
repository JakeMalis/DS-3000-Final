{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbd8d62-7b25-4590-aa1e-1395ce8b5038",
   "metadata": {},
   "source": [
    "## Phase I Project Proposal\n",
    "### Airline flight performance statistics\n",
    "\n",
    "#### Jake Malis, DS 3000 Section 4, Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e839c-1ef4-4fe0-b2dc-a0cacbe3aa6c",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd9251-8c31-4561-8085-aace74e90e76",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "I plan on using [U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics' 2015 Flight Delays and Cancellations dataset from Kaggle](https://www.kaggle.com/datasets/usdot/flight-delays) which contains airline data, airport data, and detailed flight data from 2015. This will allow me to derive at a minimum which airlines are most delayed and most cancelled. This dataset can be imported from Kaggle using [Croissant](https://github.com/mlcommons/croissant) which makes it easy to import the data for use with Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b691aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16676a7-8125-44b4-825c-fa944a551dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlcroissant\n",
      "  Downloading mlcroissant-1.0.13-py2.py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from mlcroissant) (2.32.3)\n",
      "Requirement already satisfied: pandas in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from mlcroissant) (2.2.3)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting rdflib\n",
      "  Downloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.9/564.9 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jsonpath-rw\n",
      "  Downloading jsonpath-rw-1.4.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting etils[epath]>=1.7.0\n",
      "  Downloading etils-1.12.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas-stubs\n",
      "  Downloading pandas_stubs-2.2.3.241126-py3-none-any.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from mlcroissant) (2.9.0.post0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from etils[epath]>=1.7.0->mlcroissant) (4.12.2)\n",
      "Collecting zipp\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib_resources\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting ply\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from jsonpath-rw->mlcroissant) (5.1.1)\n",
      "Requirement already satisfied: six in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from jsonpath-rw->mlcroissant) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas->mlcroissant) (2.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas->mlcroissant) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas->mlcroissant) (2024.2)\n",
      "Collecting types-pytz>=2022.1.1\n",
      "  Downloading types_pytz-2025.1.0.20250204-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from rdflib->mlcroissant) (3.2.1)\n",
      "Collecting isodate<1.0.0,>=0.7.2\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->mlcroissant) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->mlcroissant) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->mlcroissant) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->mlcroissant) (3.4.1)\n",
      "Using legacy 'setup.py install' for jsonpath-rw, since package 'wheel' is not installed.\n",
      "Installing collected packages: ply, zipp, types-pytz, tqdm, networkx, jsonpath-rw, isodate, importlib_resources, fsspec, etils, absl-py, rdflib, pandas-stubs, mlcroissant\n",
      "  Running setup.py install for jsonpath-rw ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed absl-py-2.1.0 etils-1.12.0 fsspec-2025.2.0 importlib_resources-6.5.2 isodate-0.7.2 jsonpath-rw-1.4.0 mlcroissant-1.0.13 networkx-3.4.2 pandas-stubs-2.2.3.241126 ply-3.11 rdflib-7.1.3 tqdm-4.67.1 types-pytz-2025.1.0.20250204 zipp-3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'rai', 'isLiveDataset', 'examples'}\n",
      "WARNING:absl:Found the following 1 warning(s) during the validation:\n",
      "  -  [Metadata(2015 Flight Delays and Cancellations)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RecordSet(uuid=\"airlines.csv\"), RecordSet(uuid=\"airports.csv\"), RecordSet(uuid=\"flights.csv\")]\n"
     ]
    }
   ],
   "source": [
    "! pip install mlcroissant\n",
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch the Croissant JSON-LD\n",
    "croissant_dataset = mlc.Dataset('https://www.kaggle.com/datasets/usdot/flight-delays/croissant/download')\n",
    "\n",
    "# Check what record sets are in the dataset\n",
    "record_sets = croissant_dataset.metadata.record_sets\n",
    "print(record_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "466a26f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'UA'</td>\n",
       "      <td>b'United Air Lines Inc.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'AA'</td>\n",
       "      <td>b'American Airlines Inc.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'US'</td>\n",
       "      <td>b'US Airways Inc.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'F9'</td>\n",
       "      <td>b'Frontier Airlines Inc.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'B6'</td>\n",
       "      <td>b'JetBlue Airways'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IATA_CODE                    AIRLINE\n",
       "0     b'UA'   b'United Air Lines Inc.'\n",
       "1     b'AA'  b'American Airlines Inc.'\n",
       "2     b'US'         b'US Airways Inc.'\n",
       "3     b'F9'  b'Frontier Airlines Inc.'\n",
       "4     b'B6'         b'JetBlue Airways'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_df = pd.DataFrame(croissant_dataset.records(record_set=record_sets[0].uuid))\n",
    "airlines_df.columns = airlines_df.columns.str.replace('airlines.csv/', '')\n",
    "\n",
    "airlines_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "263559f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = pd.DataFrame(croissant_dataset.records(record_set=record_sets[1].uuid))\n",
    "airports_df.columns = airports_df.columns.str.replace('airports.csv/','')\n",
    "# airports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48c60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3e25adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boqiangliang/.pyenv/versions/3.10.4/lib/python3.10/site-packages/mlcroissant/_src/operation_graph/operations/read.py:97: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flights.csv/YEAR</th>\n",
       "      <th>flights.csv/MONTH</th>\n",
       "      <th>flights.csv/DAY</th>\n",
       "      <th>flights.csv/DAY_OF_WEEK</th>\n",
       "      <th>flights.csv/AIRLINE</th>\n",
       "      <th>flights.csv/FLIGHT_NUMBER</th>\n",
       "      <th>flights.csv/TAIL_NUMBER</th>\n",
       "      <th>flights.csv/ORIGIN_AIRPORT</th>\n",
       "      <th>flights.csv/DESTINATION_AIRPORT</th>\n",
       "      <th>flights.csv/SCHEDULED_DEPARTURE</th>\n",
       "      <th>...</th>\n",
       "      <th>flights.csv/ARRIVAL_TIME</th>\n",
       "      <th>flights.csv/ARRIVAL_DELAY</th>\n",
       "      <th>flights.csv/DIVERTED</th>\n",
       "      <th>flights.csv/CANCELLED</th>\n",
       "      <th>flights.csv/CANCELLATION_REASON</th>\n",
       "      <th>flights.csv/AIR_SYSTEM_DELAY</th>\n",
       "      <th>flights.csv/SECURITY_DELAY</th>\n",
       "      <th>flights.csv/AIRLINE_DELAY</th>\n",
       "      <th>flights.csv/LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>flights.csv/WEATHER_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'AS'</td>\n",
       "      <td>98</td>\n",
       "      <td>b'N407AS'</td>\n",
       "      <td>b'ANC'</td>\n",
       "      <td>b'SEA'</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>408.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'AA'</td>\n",
       "      <td>2336</td>\n",
       "      <td>b'N3KUAA'</td>\n",
       "      <td>b'LAX'</td>\n",
       "      <td>b'PBI'</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>741.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'US'</td>\n",
       "      <td>840</td>\n",
       "      <td>b'N171US'</td>\n",
       "      <td>b'SFO'</td>\n",
       "      <td>b'CLT'</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>811.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'AA'</td>\n",
       "      <td>258</td>\n",
       "      <td>b'N3HYAA'</td>\n",
       "      <td>b'LAX'</td>\n",
       "      <td>b'MIA'</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>756.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'AS'</td>\n",
       "      <td>135</td>\n",
       "      <td>b'N527AS'</td>\n",
       "      <td>b'SEA'</td>\n",
       "      <td>b'ANC'</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>259.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flights.csv/YEAR  flights.csv/MONTH  flights.csv/DAY  \\\n",
       "0              2015                  1                1   \n",
       "1              2015                  1                1   \n",
       "2              2015                  1                1   \n",
       "3              2015                  1                1   \n",
       "4              2015                  1                1   \n",
       "\n",
       "   flights.csv/DAY_OF_WEEK flights.csv/AIRLINE  flights.csv/FLIGHT_NUMBER  \\\n",
       "0                        4               b'AS'                         98   \n",
       "1                        4               b'AA'                       2336   \n",
       "2                        4               b'US'                        840   \n",
       "3                        4               b'AA'                        258   \n",
       "4                        4               b'AS'                        135   \n",
       "\n",
       "  flights.csv/TAIL_NUMBER flights.csv/ORIGIN_AIRPORT  \\\n",
       "0               b'N407AS'                     b'ANC'   \n",
       "1               b'N3KUAA'                     b'LAX'   \n",
       "2               b'N171US'                     b'SFO'   \n",
       "3               b'N3HYAA'                     b'LAX'   \n",
       "4               b'N527AS'                     b'SEA'   \n",
       "\n",
       "  flights.csv/DESTINATION_AIRPORT  flights.csv/SCHEDULED_DEPARTURE  ...  \\\n",
       "0                          b'SEA'                                5  ...   \n",
       "1                          b'PBI'                               10  ...   \n",
       "2                          b'CLT'                               20  ...   \n",
       "3                          b'MIA'                               20  ...   \n",
       "4                          b'ANC'                               25  ...   \n",
       "\n",
       "   flights.csv/ARRIVAL_TIME  flights.csv/ARRIVAL_DELAY  flights.csv/DIVERTED  \\\n",
       "0                     408.0                      -22.0                     0   \n",
       "1                     741.0                       -9.0                     0   \n",
       "2                     811.0                        5.0                     0   \n",
       "3                     756.0                       -9.0                     0   \n",
       "4                     259.0                      -21.0                     0   \n",
       "\n",
       "   flights.csv/CANCELLED  flights.csv/CANCELLATION_REASON  \\\n",
       "0                      0                             None   \n",
       "1                      0                             None   \n",
       "2                      0                             None   \n",
       "3                      0                             None   \n",
       "4                      0                             None   \n",
       "\n",
       "   flights.csv/AIR_SYSTEM_DELAY  flights.csv/SECURITY_DELAY  \\\n",
       "0                           NaN                         NaN   \n",
       "1                           NaN                         NaN   \n",
       "2                           NaN                         NaN   \n",
       "3                           NaN                         NaN   \n",
       "4                           NaN                         NaN   \n",
       "\n",
       "   flights.csv/AIRLINE_DELAY  flights.csv/LATE_AIRCRAFT_DELAY  \\\n",
       "0                        NaN                              NaN   \n",
       "1                        NaN                              NaN   \n",
       "2                        NaN                              NaN   \n",
       "3                        NaN                              NaN   \n",
       "4                        NaN                              NaN   \n",
       "\n",
       "   flights.csv/WEATHER_DELAY  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "2                        NaN  \n",
       "3                        NaN  \n",
       "4                        NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "flights_records = list(islice(croissant_dataset.records(record_set=record_sets[2].uuid), 1000))\n",
    "flights_df = pd.DataFrame(flights_records)\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "031aa9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "column_names = flights_df.columns.tolist()\n",
    "# print(column_names[0].split('/')[1])\n",
    "print(len(flights_df.columns))\n",
    "for i in range(len(flights_df.columns)):\n",
    "    # column_names[i] = str(column_names[i]).split('/')\n",
    "    column_names[i] = str(column_names[i]).replace('flights.csv/','')\n",
    "#     # print(flights_df.columns[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ae3e803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'MONTH',\n",
       " 'DAY',\n",
       " 'DAY_OF_WEEK',\n",
       " 'AIRLINE',\n",
       " 'FLIGHT_NUMBER',\n",
       " 'TAIL_NUMBER',\n",
       " 'ORIGIN_AIRPORT',\n",
       " 'DESTINATION_AIRPORT',\n",
       " 'SCHEDULED_DEPARTURE',\n",
       " 'DEPARTURE_TIME',\n",
       " 'DEPARTURE_DELAY',\n",
       " 'TAXI_OUT',\n",
       " 'WHEELS_OFF',\n",
       " 'SCHEDULED_TIME',\n",
       " 'ELAPSED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'DISTANCE',\n",
       " 'WHEELS_ON',\n",
       " 'TAXI_IN',\n",
       " 'SCHEDULED_ARRIVAL',\n",
       " 'ARRIVAL_TIME',\n",
       " 'ARRIVAL_DELAY',\n",
       " 'DIVERTED',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_REASON',\n",
       " 'AIR_SYSTEM_DELAY',\n",
       " 'SECURITY_DELAY',\n",
       " 'AIRLINE_DELAY',\n",
       " 'LATE_AIRCRAFT_DELAY',\n",
       " 'WEATHER_DELAY']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "24b1103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(column_names)\n",
    "flights_df.columns = column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f1aeb1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>SCHEDULED_DEPARTURE</th>\n",
       "      <th>...</th>\n",
       "      <th>ARRIVAL_TIME</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_REASON</th>\n",
       "      <th>AIR_SYSTEM_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>AIRLINE_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'AS'</td>\n",
       "      <td>98</td>\n",
       "      <td>b'N407AS'</td>\n",
       "      <td>b'ANC'</td>\n",
       "      <td>b'SEA'</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>408.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'AA'</td>\n",
       "      <td>2336</td>\n",
       "      <td>b'N3KUAA'</td>\n",
       "      <td>b'LAX'</td>\n",
       "      <td>b'PBI'</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>741.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'US'</td>\n",
       "      <td>840</td>\n",
       "      <td>b'N171US'</td>\n",
       "      <td>b'SFO'</td>\n",
       "      <td>b'CLT'</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>811.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'AA'</td>\n",
       "      <td>258</td>\n",
       "      <td>b'N3HYAA'</td>\n",
       "      <td>b'LAX'</td>\n",
       "      <td>b'MIA'</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>756.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b'AS'</td>\n",
       "      <td>135</td>\n",
       "      <td>b'N527AS'</td>\n",
       "      <td>b'SEA'</td>\n",
       "      <td>b'ANC'</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>259.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  DAY  DAY_OF_WEEK AIRLINE  FLIGHT_NUMBER TAIL_NUMBER  \\\n",
       "0  2015      1    1            4   b'AS'             98   b'N407AS'   \n",
       "1  2015      1    1            4   b'AA'           2336   b'N3KUAA'   \n",
       "2  2015      1    1            4   b'US'            840   b'N171US'   \n",
       "3  2015      1    1            4   b'AA'            258   b'N3HYAA'   \n",
       "4  2015      1    1            4   b'AS'            135   b'N527AS'   \n",
       "\n",
       "  ORIGIN_AIRPORT DESTINATION_AIRPORT  SCHEDULED_DEPARTURE  ...  ARRIVAL_TIME  \\\n",
       "0         b'ANC'              b'SEA'                    5  ...         408.0   \n",
       "1         b'LAX'              b'PBI'                   10  ...         741.0   \n",
       "2         b'SFO'              b'CLT'                   20  ...         811.0   \n",
       "3         b'LAX'              b'MIA'                   20  ...         756.0   \n",
       "4         b'SEA'              b'ANC'                   25  ...         259.0   \n",
       "\n",
       "   ARRIVAL_DELAY  DIVERTED  CANCELLED  CANCELLATION_REASON  AIR_SYSTEM_DELAY  \\\n",
       "0          -22.0         0          0                 None               NaN   \n",
       "1           -9.0         0          0                 None               NaN   \n",
       "2            5.0         0          0                 None               NaN   \n",
       "3           -9.0         0          0                 None               NaN   \n",
       "4          -21.0         0          0                 None               NaN   \n",
       "\n",
       "   SECURITY_DELAY  AIRLINE_DELAY  LATE_AIRCRAFT_DELAY  WEATHER_DELAY  \n",
       "0             NaN            NaN                  NaN            NaN  \n",
       "1             NaN            NaN                  NaN            NaN  \n",
       "2             NaN            NaN                  NaN            NaN  \n",
       "3             NaN            NaN                  NaN            NaN  \n",
       "4             NaN            NaN                  NaN            NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flights_df.columns = pd.DataFrame(flights_df)\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4ee3e-875b-40af-a136-2cf7f5983e7f",
   "metadata": {},
   "source": [
    "### Data Usage and Remaining Issues\n",
    "\n",
    "The above data set consists of 3 separate datasets (airline data, airport data, and flight data) which should all be merged together in order to provide the most detailed statistics when working with the data. Also, the format for date is currently weird, and I would combine the 3 date-related columns to produce one column with date as a datatype. I think first, I would combine the 3 datasets together so that it's easier to reference all the data in one place. Then I would determine the total flights for each airline. Then I can clean up data into one table with all of the flights where they were delayed and another with all flights that were cancelled. From that, I'd be able to produce percentages about which airlines' flights are most frequently delayed and cancelled. I could also try to determine trends about which airports have the highest & lowest percentage of delayed & cancelled flights. I hypothesize that airports in cold weather states are most likely to have delays and cancellations due to weather issues, especially during winter months. I can use PyTorch to generate a model trained on half the dataset, and use it to predict the other half of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "08053004",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'origin_airport/LATITUDE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m cancelled \u001b[38;5;241m=\u001b[39m flights_df[flights_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCANCELLED\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m cancelled\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m---> 11\u001b[0m agg \u001b[38;5;241m=\u001b[39m \u001b[43mcancelled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mORIGIN_AIRPORT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morigin_airport/LATITUDE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morigin_airport/LONGITUDE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     12\u001b[0m                \u001b[38;5;241m.\u001b[39msize() \\\n\u001b[1;32m     13\u001b[0m                \u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcancellations\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Group by airport to count cancellations\u001b[39;00m\n\u001b[1;32m     17\u001b[0m agg \u001b[38;5;241m=\u001b[39m cancelled\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORIGIN_AIRPORT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLATITUDE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLONGITUDE\u001b[39m\u001b[38;5;124m'\u001b[39m]) \\\n\u001b[1;32m     18\u001b[0m                \u001b[38;5;241m.\u001b[39msize() \\\n\u001b[1;32m     19\u001b[0m                \u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcancellations\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pandas/core/frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'origin_airport/LATITUDE'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "import contextily as ctx\n",
    "\n",
    "\n",
    "\n",
    "# Filter for cancelled departures\n",
    "cancelled = flights_df[flights_df['CANCELLED'] == 1]\n",
    "\n",
    "cancelled.head()\n",
    "agg = cancelled.groupby(['ORIGIN_AIRPORT', 'origin_airport/LATITUDE', 'origin_airport/LONGITUDE']) \\\n",
    "               .size() \\\n",
    "               .reset_index(name='cancellations')\n",
    "\n",
    "\n",
    "# Group by airport to count cancellations\n",
    "agg = cancelled.groupby(['ORIGIN_AIRPORT', 'LATITUDE', 'LONGITUDE']) \\\n",
    "               .size() \\\n",
    "               .reset_index(name='cancellations')\n",
    "\n",
    "agg.head()\n",
    "\n",
    "# Create geometry from the longitude and latitude columns\n",
    "agg['geometry'] = agg.apply(lambda row: Point(row['origin_airport/LONGITUDE'], row['origin_airport/LATITUDE']), axis=1)\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(agg, geometry='geometry')\n",
    "gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Reproject to Web Mercator for contextily\n",
    "gdf_3857 = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Define the bounding box for the continental US in EPSG:4326\n",
    "# Approximate coordinates: (-125, 24) to (-66, 50)\n",
    "bbox = box(-125, 24, -66, 50)\n",
    "bbox_gdf = gpd.GeoDataFrame({'geometry': [bbox]}, crs='epsg:4326')\n",
    "bbox_3857 = bbox_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Get the bounds (xmin, ymin, xmax, ymax)\n",
    "xmin, ymin, xmax, ymax = bbox_3857.total_bounds\n",
    "\n",
    "# Plot and add basemap with extent to focus on the continental US\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "gdf_3857.plot(ax=ax, markersize=gdf_3857['cancellations']*2, color='red', alpha=0.6, legend=True)\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_title(\"Cancellation Counts at Departure Airports within Continental US\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cancellation ratio per airline and create a bar chart\n",
    "agg_flights = flights_df.groupby('AIRLINE').agg(cancel_ratio=('CANCELLED', 'mean')).reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=agg_flights, x='AIRLINE', y='cancel_ratio')\n",
    "plt.xlabel(\"Airline\")\n",
    "plt.ylabel(\"Cancellation Ratio\")\n",
    "plt.title(\"Cancellation Ratio by Airline\")\n",
    "plt.ylim(0, agg_flights['cancel_ratio'].max() * 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d6eeab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acetylesorc:$1$EV$7NtX2u0AyjUh1ZXl3pcPH0\n",
      "\n",
      "nightnippos:$1$rO$synvETLo7UWbM/P7/eTm01\n",
      "\n",
      "tacticreport:$1$E0$2UecDammfVOSb6nTEwylB0\n",
      "\n",
      "idealworman:$1$JP$0fj1TbcW6pAlvxStyREC1.\n",
      "\n",
      "apenguinlead:$1$RG$1QVQrq2tsvRHWqzi/UGTm0\n",
      "\n",
      "namefreezing:$1$wN$MQzoc5m4FAM7oys3w44ee.\n",
      "\n",
      "tenashlin:$1$xK$h4DOyTKVnCsfVHiywuNHl/\n",
      "\n",
      "shortoeve:$1$Yf$fhFRuhJ/5izRTtoDPh0Xo/\n",
      "\n",
      "softovok:$1$Fd$bGXpeaNCl72pWas3hyNDo.\n",
      "\n",
      "aimetrans:$1$Fl$vt2dvdlE19xpsNrhCQvIH/\n",
      "\n",
      "guitaristjaden:$1$Ca$9krJjtMy1dngfkUyBIYxg1\n",
      "\n",
      "culubson:$1$sk$.4EDxTAhGUAnHZZCeI3Kv1\n",
      "\n",
      "chiricsen:$1$VU$.IyfRU7Tn92rAM4Bo32gF0\n",
      "\n",
      "lynnaps:$1$be$iZyq1IwojgPcnDVuMyQD80\n",
      "\n",
      "platinumvintage:$1$Ge$sCRFJKik3avFJp2RyJ2g7.\n",
      "\n",
      "omahardee:$1$CP$0JgEl7.9HCvuc.4kFr9Ka/\n",
      "\n",
      "fortuneud:$1$Vz$7Uiygc3ibUNLfYGFdMIDi1\n",
      "\n",
      "rightsubl:$1$nd$Ac4Zg/IrfoMI825OsGoMX/\n",
      "\n",
      "nailitro:$1$wN$wYxVm/SorSRf384kqn6e2/\n",
      "\n",
      "babestahinlove:$1$ij$OrdLxOip0udGlVT6Tj7Id/\n",
      "\n",
      "2hoteditor:$1$mB$3Je3Y4ny44Vou63.IdS9h.\n",
      "\n",
      "dunawitec:$1$jN$A8Ndd7Ru9Yb3jMHge5wTc.\n",
      "\n",
      "acaderli:$1$3g$Eb8ljliePH/tsD1kQQklg.\n",
      "\n",
      "prothenorn:$1$li$JBdqMuoJxCpcpI1pe0tfy0\n",
      "\n",
      "borguitsba:$1$zt$G8kTzDx6ZIcpto6QSDpCw1\n",
      "\n",
      "thetinny:$1$aS$KmPSciEJbz8oTZ8N8MClr0\n",
      "\n",
      "epicgnome:$1$4H$Lc0PPBZ6VFKZ7Y.JXkIWI.\n",
      "\n",
      "markatoint:$1$Xq$snFHQL.tLrEQ9.oJ7Ireq/\n",
      "\n",
      "streplon:$1$CC$FpUX9dZdrcqrCpGDz74R61\n",
      "\n",
      "bigghappy:$1$55$3huy0MFjwiDIC57eyTSgg/\n",
      "\n",
      "tiltheb:$1$ci$EYWLKaFL9c2P.4z4xvsam1\n",
      "\n",
      "ravagergyps:$1$IY$uqmlk.utMUNFACVdUR.DK.\n",
      "\n",
      "twitterni:$1$3S$9yoJqdhKNuRlcDpl6jkFA/\n",
      "\n",
      "theborgitas:$1$3v$qzY5xGTKNAC4YAWxo4esx.\n",
      "\n",
      "interfail:$1$4y$f2ywUpH30pogVfH5CKidj.\n",
      "\n",
      "crayonfour:$1$0J$mDFeDSDyOM9ALT0iMzMU9.\n",
      "\n",
      "technof:$1$S1$SASo7GPwHBoIH1sSrJmvU.\n",
      "\n",
      "daysisende:$1$0r$Y0sLj6zKEXhsZW3h/9PJ.0\n",
      "\n",
      "royalcortli:$1$WL$1jGFq9NRBs1TKcKcOdZNI0\n",
      "\n",
      "aquaverit:$1$2k$fMeiE6qz70a6PTB3agW0o0\n",
      "\n",
      "tasticsupreme:$1$sG$W4q4wP1Sfa4LF1ayagW2/0\n",
      "\n",
      "aralmoni:$1$V5$1ekXsW/UjkD.hrFHaewiS1\n",
      "\n",
      "onev2:$1$OF$k/RqClsCN16cmDbOpFUbO.\n",
      "\n",
      "bobhyper:$1$1w$bHGxqOm86jOmPGIvtJGO/0\n",
      "\n",
      "liciolog:$1$5U$1avgi/NF4UjqaKSHMuNz2.\n",
      "\n",
      "baignolje:$1$Rl$94kq89f2mTErvOlDBagqq0\n",
      "\n",
      "runestnam:$1$px$wz.2J3ZE18adYGVQS6vEX/\n",
      "\n",
      "carotiended:$1$Ge$h5SpR7HaY9ZQF1qokcEhI/\n",
      "\n",
      "prodott:$1$wV$aW04VAr47N8npzWU.LEJ70\n",
      "\n",
      "sparkbear:$1$K0$IQrW05qYm1V2W0FNSTDWR.\n"
     ]
    }
   ],
   "source": [
    "text_file = open(\"hashwouser.txt\", \"r\")\n",
    "text_file_list = []\n",
    "lines = text_file.readlines()\n",
    "for line in lines:\n",
    "    print(line)\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
