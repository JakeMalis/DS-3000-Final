{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned flight data\n",
    "flights_df = pd.read_parquet(\"cleaned_flights.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hour from the 'DATE' column and create a new column 'DEPARTURE_HOUR'\n",
    "flights_df['DEPARTURE_HOUR'] = flights_df['DATE'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in 'DAILY_SNOWFALL' with 0\n",
    "flights_df['DAILY_SNOWFALL'] = flights_df['DAILY_SNOWFALL'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only delayed flights from flights_df\n",
    "delayed_flights = flights_df[flights_df['ARRIVAL_DELAY'] > 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLX Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for DEPARTURE_HOUR\n",
    "delayed_flights['HOUR_SIN'] = np.sin(2 * np.pi * delayed_flights['DEPARTURE_HOUR']/24.0)\n",
    "delayed_flights['HOUR_COS'] = np.cos(2 * np.pi * delayed_flights['DEPARTURE_HOUR']/24.0)\n",
    "\n",
    "# Example for MONTH\n",
    "delayed_flights['MONTH_SIN'] = np.sin(2 * np.pi * delayed_flights['MONTH']/12.0)\n",
    "delayed_flights['MONTH_COS'] = np.cos(2 * np.pi * delayed_flights['MONTH']/12.0)\n",
    "\n",
    "# Example for DAY_OF_WEEK (assuming 0-6 or 1-7)\n",
    "# Adjust the divisor based on your range (e.g., 7 if 0-6 or 1-7)\n",
    "delayed_flights['DAY_SIN'] = np.sin(2 * np.pi * delayed_flights['DAY_OF_WEEK']/7.0)\n",
    "delayed_flights['DAY_COS'] = np.cos(2 * np.pi * delayed_flights['DAY_OF_WEEK']/7.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = ['HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS', 'DAY_SIN', 'DAY_COS',\n",
    "            'DISTANCE', 'DAILY_SNOWFALL', 'AIRLINE', 'origin_airport/AIRPORT',\n",
    "            'destination_airport/AIRPORT']\n",
    "target = 'ARRIVAL_DELAY'\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = delayed_flights[features]\n",
    "y = delayed_flights[target]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric and categorical features\n",
    "numeric_features = ['HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS', 'DAY_SIN', 'DAY_COS',\n",
    "                    'DISTANCE', 'DAILY_SNOWFALL']\n",
    "categorical_features = ['AIRLINE', 'origin_airport/AIRPORT', 'destination_airport/AIRPORT']\n",
    "\n",
    "# Create preprocessing pipelines for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # Ignore categories present only in test set\n",
    "])\n",
    "\n",
    "# Create a column transformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing: fit on training data, transform both train and test data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert sparse matrix from OneHotEncoder to dense array if necessary\n",
    "if hasattr(X_train_processed, \"toarray\"):\n",
    "    X_train_np = X_train_processed.toarray()\n",
    "else:\n",
    "    X_train_np = X_train_processed\n",
    "\n",
    "if hasattr(X_test_processed, \"toarray\"):\n",
    "    X_test_np = X_test_processed.toarray()\n",
    "else:\n",
    "    X_test_np = X_test_processed\n",
    "\n",
    "# Convert target Series to NumPy arrays\n",
    "y_train_np = y_train.values\n",
    "y_test_np = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to MLX arrays\n",
    "X_train_mlx = mx.array(X_train_np, dtype=mx.float32)\n",
    "y_train_mlx = mx.array(y_train_np, dtype=mx.float32).reshape(-1, 1)\n",
    "X_test_mlx = mx.array(X_test_np, dtype=mx.float32)\n",
    "y_test_mlx = mx.array(y_test_np, dtype=mx.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormMLXModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, dropout_prob: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm(256) # Add BatchNorm\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm(128) # Add BatchNorm\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm(64) # Add BatchNorm\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # No dropout before final layer\n",
    "\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def __call__(self, x, training: bool = False):\n",
    "        x = self.fc1(x)\n",
    "        # Apply BatchNorm - it uses running stats during eval, updates during train\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        if training:\n",
    "            x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x) # Apply BatchNorm\n",
    "        x = self.relu2(x)\n",
    "        if training:\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x) # Apply BatchNorm\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the enhanced model\n",
    "input_dim = X_train_mlx.shape[1]\n",
    "dropout_rate = 0.2 # Example dropout rate, tune as needed\n",
    "mlx_model = BatchNormMLXModel(input_dim, dropout_prob=dropout_rate)\n",
    "mx.eval(mlx_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(model, x, y):\n",
    "    # Pass training=True to enable dropout during loss calculation for training\n",
    "    return mx.mean((model(x, training=True) - y) ** 2)\n",
    "\n",
    "# Loss and gradient function remains the same conceptually\n",
    "loss_and_grad_fn = nn.value_and_grad(mlx_model, mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for MLX model\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "num_samples = X_train_mlx.shape[0] # Use training data size\n",
    "num_batches_per_epoch = (num_samples + batch_size - 1) // batch_size\n",
    "total_steps = epochs * num_batches_per_epoch\n",
    "initial_lr = 0.001 # Your starting LR\n",
    "lr_schedule = optim.cosine_decay(initial_lr, total_steps)\n",
    "optimizer = optim.AdamW(learning_rate=lr_schedule, weight_decay=0.01) # Use schedule\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    mlx_model.train() # Set model to training mode (enables dropout)\n",
    "    permutation = np.random.permutation(num_samples)\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_indices = permutation[i:i+batch_size]\n",
    "        batch_indices_mlx = mx.array(batch_indices, dtype=mx.int32)\n",
    "        X_batch = X_train_mlx[batch_indices_mlx]\n",
    "        y_batch = y_train_mlx[batch_indices_mlx]\n",
    "\n",
    "        # Loss and gradients calculated with dropout enabled via mse_loss -> model(..., training=True)\n",
    "        loss, grads = loss_and_grad_fn(mlx_model, X_batch, y_batch)\n",
    "\n",
    "        optimizer.update(mlx_model, grads)\n",
    "        mx.eval(mlx_model.parameters(), optimizer.state)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MLX model on the test set\n",
    "print(\"Evaluating MLX model on test set...\")\n",
    "mlx_model.eval() # Set model to evaluation mode (disables dropout)\n",
    "# Pass training=False to disable dropout during evaluation\n",
    "y_pred_test_mlx = mlx_model(X_test_mlx, training=False)\n",
    "mx.eval(y_pred_test_mlx)\n",
    "\n",
    "y_pred_test_mlx_np = np.array(y_pred_test_mlx.squeeze())\n",
    "\n",
    "mlx_mse_test = mean_squared_error(y_test_np, y_pred_test_mlx_np)\n",
    "mlx_r2_test = r2_score(y_test_np, y_pred_test_mlx_np)\n",
    "\n",
    "print(f\"MLX Test Mean Squared Error (MSE): {mlx_mse_test}\")\n",
    "print(f\"MLX Test R² Score: {mlx_r2_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
