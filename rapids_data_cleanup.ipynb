{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e27d6393",
      "metadata": {
        "id": "e27d6393"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "726031c3",
      "metadata": {
        "id": "726031c3"
      },
      "source": [
        "### Get US DOT Flight Delay Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf52da1",
      "metadata": {
        "id": "9cf52da1"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"usdot/flight-delays\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3ea659",
      "metadata": {
        "id": "bd3ea659"
      },
      "outputs": [],
      "source": [
        "import cudf\n",
        "\n",
        "airlines_df = cudf.read_csv(path + \"/airlines.csv\")\n",
        "airports_df = cudf.read_csv(path + \"/airports.csv\")\n",
        "flights_df = cudf.read_csv(path + \"/flights.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flights_df.head()"
      ],
      "metadata": {
        "id": "Ik9kNQBfLxcE"
      },
      "id": "Ik9kNQBfLxcE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjUKzoT6KS6H"
      },
      "source": [
        "### Get NOAA Weather Station Data"
      ],
      "id": "mjUKzoT6KS6H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ytIZ-4OKS6H"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "# URL of the file\n",
        "url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.csv\"\n",
        "file = \"ghcnd-stations.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url, stream=True)\n",
        "with open(file, \"wb\") as f:\n",
        "    shutil.copyfileobj(response.raw, f)"
      ],
      "id": "9ytIZ-4OKS6H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4iVtOpNKS6H"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "FORMAT OF \"ghcnd-stations.txt\"\n",
        "\n",
        "------------------------------\n",
        "Variable   Columns   Type\n",
        "------------------------------\n",
        "ID            1-11   Character\n",
        "LATITUDE     13-20   Real\n",
        "LONGITUDE    22-30   Real\n",
        "ELEVATION    32-37   Real\n",
        "STATE        39-40   Character\n",
        "NAME         42-71   Character\n",
        "GSN FLAG     73-75   Character\n",
        "HCN/CRN FLAG 77-79   Character\n",
        "WMO ID       81-85   Character\n",
        "------------------------------\n",
        "'''\n",
        "ghcnd_stations_df = cudf.read_csv(\n",
        "    file,\n",
        "    header=None,\n",
        "    names=[\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"],\n",
        "    usecols=[0, 1, 2, 3],  # Select only the columns we need\n",
        "    skipinitialspace=True\n",
        ")\n",
        "ghcnd_stations_df.head()\n"
      ],
      "id": "s4iVtOpNKS6H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKcXy9oFKS6H"
      },
      "source": [
        "### Add in NOAA Weather Station IDs for airports"
      ],
      "id": "pKcXy9oFKS6H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ypuznr4KS6H"
      },
      "source": [
        "I have a DataFrame, `ghcnd_stations_df` that has a list of weather data stations with their lat & long stored as columns (`LATITUDE`, `LONGITUDE`). I want to use the `airports_df` which also has `LATITUDE` and `LONGITUDE` columns to find the nearest weather station to each airport. Append the `ghcnd_stations_df` `ID` column to `airports_df` as `NOAA_STATION_ID`."
      ],
      "id": "_ypuznr4KS6H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK4L_OkvKS6H"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "from cuml.neighbors import NearestNeighbors\n",
        "\n",
        "# Remove rows with NaN lat/lon in either DataFrame\n",
        "airports_df = airports_df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
        "ghcnd_stations_df = ghcnd_stations_df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
        "\n",
        "# Convert airport/station lat-lon to radians using CuPy\n",
        "airports_df['lat_rad'] = cp.radians(airports_df['LATITUDE'].to_cupy())\n",
        "airports_df['lon_rad'] = cp.radians(airports_df['LONGITUDE'].to_cupy())\n",
        "ghcnd_stations_df['lat_rad'] = cp.radians(ghcnd_stations_df['LATITUDE'].to_cupy())\n",
        "ghcnd_stations_df['lon_rad'] = cp.radians(ghcnd_stations_df['LONGITUDE'].to_cupy())\n",
        "\n",
        "# Build the NearestNeighbors model from cuML\n",
        "stations_coords = ghcnd_stations_df[['lat_rad','lon_rad']].to_cupy()\n",
        "nn = NearestNeighbors(n_neighbors=1, metric='haversine')\n",
        "nn.fit(stations_coords)\n",
        "\n",
        "# Query nearest station for each airport\n",
        "airports_coords = airports_df[['lat_rad','lon_rad']].to_cupy()\n",
        "distances, indices = nn.kneighbors(airports_coords)\n",
        "\n",
        "# Append the station ID to airports_df\n",
        "airports_df['NOAA_STATION_ID'] = ghcnd_stations_df['ID'].iloc[indices.flatten().get()].to_numpy()\n",
        "\n",
        "# Remove the temporary columns used for calculations\n",
        "airports_df.drop(columns=['lat_rad', 'lon_rad'], inplace=True)"
      ],
      "id": "wK4L_OkvKS6H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbejPjv8KS6H"
      },
      "outputs": [],
      "source": [
        "airports_df.head()"
      ],
      "id": "VbejPjv8KS6H"
    },
    {
      "cell_type": "markdown",
      "id": "75a59887",
      "metadata": {
        "id": "75a59887"
      },
      "source": [
        "## Data Cleanup\n",
        "\n",
        "Merges the 3 DataFrames together, first by removing their file name prefixes from the column names. Then, by adding in airport information for both the origin and destination airport (merges the Airports DF twice technically), and then joins the airlines table with the flights table. Lastly, we create a datetime object from the existing date columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536c96d7",
      "metadata": {
        "id": "536c96d7"
      },
      "outputs": [],
      "source": [
        "# Remove file name prefix from column names\n",
        "airlines_df.columns = airlines_df.columns.str.replace(r'^airlines\\.csv/', '', regex=True)\n",
        "airlines_df.rename(columns={'AIRLINE': 'AIRLINE NAME'}, inplace=True)\n",
        "\n",
        "airports_df.columns = airports_df.columns.str.replace(r'^airports\\.csv/', '', regex=True)\n",
        "\n",
        "flights_df.columns = flights_df.columns.str.replace(r'^flights\\.csv/', '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ae67c7",
      "metadata": {
        "id": "62ae67c7"
      },
      "outputs": [],
      "source": [
        "# Join airlines data to flights table\n",
        "flights_df = flights_df.merge(airlines_df, left_on='AIRLINE', right_on='IATA_CODE', how='left')\n",
        "# Remove duplicate IATA_CODE column if it exists\n",
        "flights_df = flights_df.drop(columns=['IATA_CODE'], errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56234dab",
      "metadata": {
        "id": "56234dab"
      },
      "outputs": [],
      "source": [
        "# Join airports data to flights table\n",
        "origin_airports = airports_df.add_prefix('origin_airport/')\n",
        "destination_airports = airports_df.add_prefix('destination_airport/')\n",
        "\n",
        "# Rename the index in origin_airports to match the column in flights_df\n",
        "origin_airports = origin_airports.rename(columns={'origin_airport/IATA_CODE': 'ORIGIN_AIRPORT'})\n",
        "flights_df = flights_df.merge(origin_airports, on='ORIGIN_AIRPORT', how='left')\n",
        "\n",
        "\n",
        "# Rename the index in destination_airports to match the column in flights_df\n",
        "destination_airports = destination_airports.rename(columns={'destination_airport/IATA_CODE': 'DESTINATION_AIRPORT'})\n",
        "flights_df = flights_df.merge(destination_airports, on='DESTINATION_AIRPORT', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the columns to match the expected format\n",
        "flights_df.rename(\n",
        "    columns={\n",
        "        \"YEAR\": \"year\",\n",
        "        \"MONTH\": \"month\",\n",
        "        \"DAY\": \"day\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "# Try creating the 'DATE' column again\n",
        "flights_df['DATE'] = cudf.to_datetime(flights_df[['year', 'month', 'day']])\n",
        "\n",
        "# Optionally, rename columns back to original names\n",
        "flights_df.rename(\n",
        "    columns={\n",
        "        \"year\": \"YEAR\",\n",
        "        \"month\": \"MONTH\",\n",
        "        \"day\": \"DAY\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")"
      ],
      "metadata": {
        "id": "MZKS-OS1ZjBH"
      },
      "id": "MZKS-OS1ZjBH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4ba33c",
      "metadata": {
        "id": "4d4ba33c"
      },
      "outputs": [],
      "source": [
        "# Replace the values in the CANCELLATION_REASON column\n",
        "flights_df['CANCELLATION_REASON'] = flights_df['CANCELLATION_REASON'].replace({\n",
        "    'A': 'Airline/Carrier',\n",
        "    'B': 'Weather',\n",
        "    'C': 'National Air System',\n",
        "    'D': 'Security'\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d0c23e0",
      "metadata": {
        "id": "5d0c23e0"
      },
      "source": [
        "### Get Weather Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95af1d02",
      "metadata": {
        "id": "95af1d02"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "# URL of the file\n",
        "url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/2015.csv.gz\"\n",
        "output_gz_file = \"2015.csv.gz\"\n",
        "output_csv_file = \"2015.csv\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url, stream=True)\n",
        "with open(output_gz_file, \"wb\") as f:\n",
        "    shutil.copyfileobj(response.raw, f)\n",
        "\n",
        "# Extract the gzip file\n",
        "with gzip.open(output_gz_file, \"rb\") as f_in:\n",
        "    with open(output_csv_file, \"wb\") as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "print(f\"File downloaded and extracted to {output_csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6307619",
      "metadata": {
        "id": "d6307619"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "ID = 11 character station identification code\n",
        "YEAR/MONTH/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)\n",
        "ELEMENT = 4 character indicator of element type\n",
        "DATA VALUE = 5 character data value for ELEMENT\n",
        "M-FLAG = 1 character Measurement Flag\n",
        "Q-FLAG = 1 character Quality Flag\n",
        "S-FLAG = 1 character Source Flag\n",
        "OBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am)\n",
        "'''\n",
        "weather_df = cudf.read_csv(output_csv_file,\n",
        "                         header=None,\n",
        "                         names=['ID', 'YEAR/MONTH/DAY',\n",
        "                                'ELEMENT', 'DATA_VALUE',\n",
        "                                'M_FLAG', 'Q_FLAG', 'S_FLAG',\n",
        "                                'OBS_TIME'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde04d41",
      "metadata": {
        "id": "bde04d41"
      },
      "outputs": [],
      "source": [
        "# Convert YEAR/MONTH/DAY column to Date type\n",
        "weather_df['DATE'] = cudf.to_datetime(weather_df['YEAR/MONTH/DAY'], format='%Y%m%d')\n",
        "weather_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5fc585",
      "metadata": {
        "id": "9c5fc585"
      },
      "source": [
        "### Add snowfall data to flight data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea0b89c",
      "metadata": {
        "id": "1ea0b89c"
      },
      "outputs": [],
      "source": [
        "# Filter daily snowfall records\n",
        "snow_df = weather_df[weather_df['ELEMENT'] == 'SNOW'].copy()\n",
        "# Convert YEAR/MONTH/DAY column in snow_df to cudf.Timestamp, and rename it to SNOW_DATE\n",
        "# Ensure SNOW_DATE is of type cudf.Timestamp\n",
        "snow_df['SNOW_DATE'] = cudf.to_datetime(snow_df['YEAR/MONTH/DAY'].astype(str))\n",
        "\n",
        "# Convert 'DATE' column in flights_df to have the same dtype as 'SNOW_DATE'\n",
        "flights_df['DATE'] = flights_df['DATE'].astype(snow_df['SNOW_DATE'].dtype)\n",
        "\n",
        "# Merge on DATE_ONLY to match daily snowfall\n",
        "flights_df = flights_df.merge(\n",
        "    snow_df[['ID', 'SNOW_DATE', 'DATA_VALUE']], # Use DATA_VALUE instead of DAILY_SNOWFALL\n",
        "    left_on=['origin_airport/NOAA_STATION_ID', 'DATE'],\n",
        "    right_on=['ID', 'SNOW_DATE'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Clean up columns no longer needed\n",
        "flights_df.drop(columns=['ID', 'SNOW_DATE'], inplace=True)\n",
        "\n",
        "# Rename DATA_VALUE to DAILY_SNOWFALL for clarity\n",
        "flights_df.rename(columns={'DATA_VALUE': 'DAILY_SNOWFALL'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda9037e",
      "metadata": {
        "id": "eda9037e"
      },
      "source": [
        "### Export Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del airlines_df\n",
        "del airports_df\n",
        "del origin_airports\n",
        "del destination_airports\n",
        "del ghcnd_stations_df\n",
        "del weather_df\n",
        "del snow_df"
      ],
      "metadata": {
        "id": "vNT5ZJb1jJBb"
      },
      "id": "vNT5ZJb1jJBb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df4b4bfc",
      "metadata": {
        "id": "df4b4bfc"
      },
      "outputs": [],
      "source": [
        "# Export cleaned data to Parquet file\n",
        "flights_df.to_parquet(\"cleaned_flights.parquet\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}