{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYeGwsGlHn4r"
      },
      "source": [
        "# Flight Delay Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGU783tIHn4r"
      },
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLM6nf18Hn4s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufnWNFkcHn4s"
      },
      "outputs": [],
      "source": [
        "# Load the cleaned flight data\n",
        "flights_df = pd.read_parquet(\"https://github.com/JakeMalis/DS-3000-Final/raw/refs/heads/main/cleaned_flights.parquet\").sample(frac=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08P9Xda7Hn4s"
      },
      "outputs": [],
      "source": [
        "# Extract the hour from the 'DATE' column and create a new column 'DEPARTURE_HOUR'\n",
        "flights_df['DEPARTURE_HOUR'] = flights_df['DATE'].dt.hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIKLE8sUHn4s"
      },
      "outputs": [],
      "source": [
        "# Replace missing values in 'DAILY_SNOWFALL' with 0\n",
        "flights_df['DAILY_SNOWFALL'] = flights_df['DAILY_SNOWFALL'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values in 'ARRIVAL_DELAY' with 0\n",
        "flights_df['ARRIVAL_DELAY'] = flights_df['ARRIVAL_DELAY'].fillna(0)"
      ],
      "metadata": {
        "id": "lc7ltPIlO2Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDK7Mnm3Hn4s"
      },
      "source": [
        "## JAX Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwJcjTTaHn4s"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import random, grad, jit, value_and_grad\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9VcgKxMHn4t"
      },
      "outputs": [],
      "source": [
        "# Standardize numeric columns for PyTorch models\n",
        "for col in ['MONTH', 'DEPARTURE_HOUR', 'DAY_OF_WEEK', 'DISTANCE']:\n",
        "    col_mean = flights_df[col].mean()\n",
        "    col_std  = flights_df[col].std()\n",
        "    flights_df[col] = (flights_df[col] - col_mean) / col_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AiVqdZEHn4t"
      },
      "outputs": [],
      "source": [
        "# Extract features for JAX models\n",
        "numeric_feats = flights_df[['MONTH', 'DEPARTURE_HOUR', 'DAY_OF_WEEK', 'DISTANCE', 'DAILY_SNOWFALL']].astype(np.float32).values # Use numpy\n",
        "categorical_feats = pd.get_dummies(flights_df[['AIRLINE', 'origin_airport/AIRPORT', 'destination_airport/AIRPORT']]).values.astype(np.float32) # Use numpy\n",
        "X = np.hstack([numeric_feats, categorical_feats])\n",
        "y = flights_df['ARRIVAL_DELAY'].values.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMtUflsVHn4t"
      },
      "outputs": [],
      "source": [
        "# Initialize TPU for JAX\n",
        "jax_devices = jax.devices(\"tpu\")\n",
        "print(f\"Using device: {jax_devices[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UT9GSABHn4t"
      },
      "outputs": [],
      "source": [
        "# Split data into train/test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "test_size = len(X) - train_size\n",
        "\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Function to create batches\n",
        "def create_batches(X, y, batch_size):\n",
        "    n_batches = len(X) // batch_size\n",
        "    for i in range(n_batches):\n",
        "        yield X[i * batch_size: (i + 1) * batch_size], y[i * batch_size: (i + 1) * batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e85-RabvHn4t"
      },
      "outputs": [],
      "source": [
        "# Define Feedforward Neural Network in JAX\n",
        "class FeedForwardNN:\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, key):\n",
        "        # Initialize weights and biases with a PRNGKey\n",
        "        keys = random.split(key, 3)\n",
        "        self.W1 = random.normal(keys[0], (input_size, hidden_size1)) * jnp.sqrt(2.0 / input_size)\n",
        "        self.b1 = jnp.zeros(hidden_size1)\n",
        "        self.W2 = random.normal(keys[1], (hidden_size1, hidden_size2)) * jnp.sqrt(2.0 / hidden_size1)\n",
        "        self.b2 = jnp.zeros(hidden_size2)\n",
        "        self.W3 = random.normal(keys[2], (hidden_size2, output_size)) * jnp.sqrt(2.0 / hidden_size2)\n",
        "        self.b3 = jnp.zeros(output_size)\n",
        "\n",
        "    # Function to get trainable parameters\n",
        "    def get_params(self):\n",
        "        return [(self.W1, self.b1), (self.W2, self.b2), (self.W3, self.b3)]\n",
        "\n",
        "    # Function to set trainable parameters\n",
        "    def set_params(self, params):\n",
        "        self.W1, self.b1 = params[0]\n",
        "        self.W2, self.b2 = params[1]\n",
        "        self.W3, self.b3 = params[2]\n",
        "\n",
        "    def forward(self, X):\n",
        "        # First hidden layer\n",
        "        hidden1 = jax.nn.relu(jnp.dot(X, self.W1) + self.b1)\n",
        "        # Second hidden layer\n",
        "        hidden2 = jax.nn.relu(jnp.dot(hidden1, self.W2) + self.b2)\n",
        "        # Output layer\n",
        "        output = jnp.dot(hidden2, self.W3) + self.b3\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doVOo7SfHn4t"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "input_size = X.shape[1]\n",
        "hidden_size1 = 64  # Example hidden layer size\n",
        "hidden_size2 = 32   # Example second hidden layer size\n",
        "output_size = 1     # Predicting a single value (arrival delay)\n",
        "\n",
        "rng_key = random.PRNGKey(0) # Define the initial random key\n",
        "model = FeedForwardNN(input_size, hidden_size1, hidden_size2, output_size, rng_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function - takes params as the first argument for grad\n",
        "def mse_loss(params, X, y):\n",
        "    # Create a dummy model instance to use the forward method\n",
        "    # The actual parameters are passed in `params`\n",
        "    dummy_model = FeedForwardNN(params[0][0].shape[0], params[0][0].shape[1], params[1][0].shape[1], params[2][0].shape[1], random.PRNGKey(0))\n",
        "    dummy_model.set_params(params) # Set the current parameters\n",
        "\n",
        "    preds = dummy_model.forward(X)\n",
        "    return jnp.mean((preds - y) ** 2)\n",
        "\n",
        "# Predict function - takes params and X, converts X to JAX array\n",
        "@jit\n",
        "def predict(params, X):\n",
        "    dummy_model = FeedForwardNN(params[0][0].shape[0], params[0][0].shape[1], params[1][0].shape[1], params[2][0].shape[1], random.PRNGKey(0))\n",
        "    dummy_model.set_params(params)\n",
        "    return dummy_model.forward(X)\n",
        "\n",
        "# Define the update step (train_step)\n",
        "@jit\n",
        "def train_step(params, X_batch, y_batch, learning_rate):\n",
        "    \"\"\"Updates model parameters using gradient descent on a batch.\"\"\"\n",
        "    loss, grads = value_and_grad(mse_loss)(params, X_batch, y_batch)\n",
        "    # Update parameters\n",
        "    updated_params = []\n",
        "    for param, grad in zip(params, grads):\n",
        "        updated_params.append((param[0] - learning_rate * grad[0],\n",
        "                               param[1] - learning_rate * grad[1]))\n",
        "    return updated_params"
      ],
      "metadata": {
        "id": "JIl9IvqK8zgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc3DJ7bqHn4t"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "params = model.get_params() # Get initial parameters from the model instance\n",
        "num_epochs = 20\n",
        "learning_rate = 0.00001\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for X_batch_np, y_batch_np in create_batches(X_train, y_train, batch_size):\n",
        "        # Convert batch to JAX arrays before passing to train_step\n",
        "        X_batch_jax = jnp.array(X_batch_np, dtype=jnp.float32)\n",
        "        y_batch_jax = jnp.array(y_batch_np, dtype=jnp.float32).reshape(-1, 1)\n",
        "        params = train_step(params, X_batch_jax, y_batch_jax, learning_rate)\n",
        "\n",
        "    # Calculate training loss on a subset of the data to reduce memory usage\n",
        "    # Convert subset to JAX array before calculating loss\n",
        "    X_train_subset_jax = jnp.array(X_train[:10000], dtype=jnp.float32)\n",
        "    y_train_subset_jax = jnp.array(y_train[:10000], dtype=jnp.float32).reshape(-1, 1)\n",
        "    train_loss = mse_loss(params, X_train_subset_jax, y_train_subset_jax)\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sy6EsfVHn4t"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "# Convert test set to JAX array for prediction\n",
        "X_test_jax = jnp.array(X_test, dtype=jnp.float32)\n",
        "y_pred = predict(params, X_test_jax)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = jnp.sqrt(mse)\n",
        "\n",
        "print(f\"Test R-squared (R2): {r2:.4f}\")\n",
        "print(f\"Test Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Test Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V6E1",
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}