{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27d6393",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726031c3",
   "metadata": {},
   "source": [
    "### Get US DOT Flight Delay Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf52da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"usdot/flight-delays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ea659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "airlines_df = pd.read_csv(path + \"/airlines.csv\", low_memory=False, encoding='UTF-8')\n",
    "airports_df = pd.read_csv(path + \"/airports.csv\", low_memory=False, encoding='UTF-8')\n",
    "flights_df = pd.read_csv(path + \"/flights.csv\", low_memory=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NOAA Weather Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# URL of the file\n",
    "url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.csv\"\n",
    "file = \"ghcnd-stations.csv\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url, stream=True)\n",
    "with open(file, \"wb\") as f:\n",
    "    shutil.copyfileobj(response.raw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FORMAT OF \"ghcnd-stations.txt\"\n",
    "\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSN FLAG     73-75   Character\n",
    "HCN/CRN FLAG 77-79   Character\n",
    "WMO ID       81-85   Character\n",
    "------------------------------\n",
    "'''\n",
    "ghcnd_stations_df = pd.read_csv(\n",
    "    file,\n",
    "    header=None,\n",
    "    names=[\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"STATE\", \"NAME\", \n",
    "           \"GSN_FLAG\", \"HCN_CRN_FLAG\", \"WMO_ID\"],\n",
    "    usecols=[0, 1, 2, 3, 4, 5, 6, 7, 8],  # Select only the columns we need\n",
    "    dtype={\"ID\": str},\n",
    "    skipinitialspace=True\n",
    ")\n",
    "ghcnd_stations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in NOAA Weather Station IDs for airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a DataFrame, `ghcnd_stations_df` that has a list of weather data stations with their lat & long stored as columns (`LATITUDE`, `LONGITUDE`). I want to use the `airports_df` which also has `LATITUDE` and `LONGITUDE` columns to find the nearest weather station to each airport. Append the `ghcnd_stations_df` `ID` column to `airports_df` as `NOAA_STATION_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Remove rows with NaN lat/lon in either DataFrame\n",
    "airports_df = airports_df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "ghcnd_stations_df = ghcnd_stations_df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "\n",
    "# Convert airport/station lat-lon to radians\n",
    "airports_df['lat_rad'] = np.radians(airports_df['LATITUDE'])\n",
    "airports_df['lon_rad'] = np.radians(airports_df['LONGITUDE'])\n",
    "ghcnd_stations_df['lat_rad'] = np.radians(ghcnd_stations_df['LATITUDE'])\n",
    "ghcnd_stations_df['lon_rad'] = np.radians(ghcnd_stations_df['LONGITUDE'])\n",
    "\n",
    "# Build the tree from stations\n",
    "stations_coords = ghcnd_stations_df[['lat_rad','lon_rad']].to_numpy()\n",
    "tree = BallTree(stations_coords, metric='haversine')\n",
    "\n",
    "# Query nearest station for each airport\n",
    "airports_coords = airports_df[['lat_rad','lon_rad']].to_numpy()\n",
    "distances, indices = tree.query(airports_coords, k=1)\n",
    "\n",
    "# Append the station ID to airports_df\n",
    "airports_df['NOAA_STATION_ID'] = ghcnd_stations_df.iloc[indices.flatten()]['ID'].values\n",
    "\n",
    "# Remove the temporary columns used for calculations\n",
    "airports_df.drop(columns=['lat_rad', 'lon_rad'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a59887",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "Merges the 3 DataFrames together, first by removing their file name prefixes from the column names. Then, by adding in airport information for both the origin and destination airport (merges the Airports DF twice technically), and then joins the airlines table with the flights table. Lastly, we create a datetime object from the existing date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove file name prefix from column names\n",
    "airlines_df.columns = airlines_df.columns.str.replace(r'^airlines\\.csv/', '', regex=True)\n",
    "airlines_df.rename(columns={'AIRLINE': 'AIRLINE NAME'}, inplace=True)\n",
    "\n",
    "airports_df.columns = airports_df.columns.str.replace(r'^airports\\.csv/', '', regex=True)\n",
    "\n",
    "flights_df.columns = flights_df.columns.str.replace(r'^flights\\.csv/', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join airlines data to flights table\n",
    "flights_df = flights_df.join(airlines_df.set_index('IATA_CODE'), on='AIRLINE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56234dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join airports data to flights table\n",
    "origin_airports = airports_df.add_prefix('origin_airport/')\n",
    "destination_airports = airports_df.add_prefix('destination_airport/')\n",
    "\n",
    "flights_df = flights_df.join(origin_airports.set_index('origin_airport/IATA_CODE'), on='ORIGIN_AIRPORT')\n",
    "flights_df = flights_df.join(destination_airports.set_index('destination_airport/IATA_CODE'), on='DESTINATION_AIRPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DATE column from YEAR, MONTH, DAY, and SCHEDULED_DEPARTURE columns\n",
    "flights_df['DATE'] = pd.to_datetime(\n",
    "    flights_df['YEAR'].astype(str) + '-' +\n",
    "    flights_df['MONTH'].astype(str) + '-' +\n",
    "    flights_df['DAY'].astype(str) + ' ' +\n",
    "    flights_df['SCHEDULED_DEPARTURE'].astype(str).str.zfill(4),\n",
    "    format='%Y-%m-%d %H%M'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ba33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the CANCELLATION_REASON column\n",
    "flights_df['CANCELLATION_REASON'] = flights_df['CANCELLATION_REASON'].replace({\n",
    "    'A': 'Airline/Carrier',\n",
    "    'B': 'Weather',\n",
    "    'C': 'National Air System',\n",
    "    'D': 'Security'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0c23e0",
   "metadata": {},
   "source": [
    "### Get Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# URL of the file\n",
    "url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/2015.csv.gz\"\n",
    "output_gz_file = \"2015.csv.gz\"\n",
    "output_csv_file = \"2015.csv\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url, stream=True)\n",
    "with open(output_gz_file, \"wb\") as f:\n",
    "    shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "# Extract the gzip file\n",
    "with gzip.open(output_gz_file, \"rb\") as f_in:\n",
    "    with open(output_csv_file, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f\"File downloaded and extracted to {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6307619",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ID = 11 character station identification code\n",
    "YEAR/MONTH/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)\n",
    "ELEMENT = 4 character indicator of element type \n",
    "DATA VALUE = 5 character data value for ELEMENT \n",
    "M-FLAG = 1 character Measurement Flag \n",
    "Q-FLAG = 1 character Quality Flag \n",
    "S-FLAG = 1 character Source Flag \n",
    "OBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am)\n",
    "'''\n",
    "weather_df = pd.read_csv(output_csv_file,\n",
    "                         header=None,\n",
    "                         names=['ID', 'YEAR/MONTH/DAY',\n",
    "                                'ELEMENT', 'DATA_VALUE',\n",
    "                                'M_FLAG', 'Q_FLAG', 'S_FLAG',\n",
    "                                'OBS_TIME'],\n",
    "                         low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde04d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert YEAR/MONTH/DAY column to Date type\n",
    "weather_df['DATE'] = pd.to_datetime(weather_df['YEAR/MONTH/DAY'], format='%Y%m%d')\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fc585",
   "metadata": {},
   "source": [
    "### Add snowfall data to flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter daily snowfall records\n",
    "snow_df = weather_df[weather_df['ELEMENT'] == 'SNOW'].copy()\n",
    "snow_df.rename(columns={'DATA_VALUE': 'DAILY_SNOWFALL'}, inplace=True)\n",
    "\n",
    "# Temporarily create a date-only column\n",
    "flights_df['DATE_ONLY'] = flights_df['DATE'].dt.floor('d')\n",
    "\n",
    "# Rename the DATE column in snow_df to avoid conflicts\n",
    "snow_df.rename(columns={'DATE': 'SNOW_DATE'}, inplace=True)\n",
    "\n",
    "# Merge on DATE_ONLY to match daily snowfall\n",
    "flights_df = flights_df.merge(\n",
    "    snow_df[['ID', 'SNOW_DATE', 'DAILY_SNOWFALL']],\n",
    "    left_on=['origin_airport/NOAA_STATION_ID', 'DATE_ONLY'],\n",
    "    right_on=['ID', 'SNOW_DATE'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up columns no longer needed\n",
    "flights_df.drop(columns=['DATE_ONLY', 'ID', 'SNOW_DATE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9037e",
   "metadata": {},
   "source": [
    "### Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to Parquet file\n",
    "flights_df.to_parquet(\"cleaned_flights.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
